{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Using-the-CPSC-API-(unsuccessfully)\" data-toc-modified-id=\"Using-the-CPSC-API-(unsuccessfully)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Using the CPSC API (unsuccessfully)</a></div><div class=\"lev1 toc-item\"><a href=\"#Cleaning-up-Raw-Data\" data-toc-modified-id=\"Cleaning-up-Raw-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Cleaning up Raw Data</a></div><div class=\"lev2 toc-item\"><a href=\"#API-Data\" data-toc-modified-id=\"API-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>API Data</a></div><div class=\"lev2 toc-item\"><a href=\"#NEISS-Data\" data-toc-modified-id=\"NEISS-Data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>NEISS Data</a></div><div class=\"lev1 toc-item\"><a href=\"#Beginning-of-Analysis\" data-toc-modified-id=\"Beginning-of-Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Beginning of Analysis</a></div><div class=\"lev3 toc-item\"><a href=\"#Natural-Language-Processing-Approach\" data-toc-modified-id=\"Natural-Language-Processing-Approach-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Natural Language Processing Approach</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-18T16:19:20.667281",
     "start_time": "2016-09-18T16:19:20.663198"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unirest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the CPSC API (unsuccessfully)\n",
    "\n",
    "Here I try to make a request from the CPSC API and it returns a blank even though the status code is 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = os.environ.get('CPSC_KEY')\n",
    "resp = unirest.get('http://www.saferproducts.gov/webapi/Cpsc.Cpsrms.Web.Api.svc/',\n",
    "                   auth=(key,''), headers={\"Accept\": \"application/json\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up Raw Data\n",
    "\n",
    "## API Data\n",
    "Rather than calling from the API, we received the data from the epidemiologist from the CPSC in a raw `.txt` format. However it is a JSON file and there are levels of nesting that we would need to parse out so I created this script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('data/raw_api_data.txt')\n",
    "\n",
    "cols_to_parse = ['Gender', 'SeverityType', 'Locale', 'ProductCategory']\n",
    "cols_to_add = [['GenderDescription','GenderId','GenderPublicName'],\n",
    "['IncidentDetails','SeverityTypeDescription','SeverityTypePublicName'],\n",
    "['LocaleDescription','LocalePublicName'],\n",
    "['ProductCategoryDescription','ProductCategoryPublicName']]\n",
    "new_df = pd.DataFrame()\n",
    "cols_dict = dict(zip(cols_to_parse, cols_to_add))\n",
    "\n",
    "for key in cols_dict:\n",
    "    placeholder = pd.DataFrame(columns = cols_dict[key])\n",
    "    for (i, row) in data.iterrows():\n",
    "        e = row[key]\n",
    "        value_holder = []\n",
    "        for item in cols_dict[key]:\n",
    "            try:\n",
    "                component = e[item]\n",
    "            except:\n",
    "                component = 'Missing'\n",
    "            value_holder.append(component)\n",
    "        placeholder.loc[i, :] = value_holder\n",
    "    if new_df.shape[0] == 0:\n",
    "        new_df = placeholder\n",
    "    else:\n",
    "        new_df = pd.concat([new_df, placeholder], axis=1)\n",
    "        \n",
    "new_df2 = pd.concat([data, new_df], axis=1)\n",
    "new_df2 = new_df2.drop(['CompanyComments', 'Gender','IncidentDocuments','IncidentDetails', 'Locale', 'ProductCategory',\n",
    "                      'RelationshipType','SeverityType', 'SourceType'], axis=1)\n",
    "new_df2.to_pickle('data/cleaned_api_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEISS Data\n",
    "\n",
    "Combining all the NEISS data from 2009. 2015 was different formatted so I changed it manually and combined all the files into a single data frame. The raw NEISS data wasn't so bad to work with but I figure we could compress it just to save space in the `processed` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-18T14:25:21.366071",
     "start_time": "2016-09-18T14:25:21.362378"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compressing raw data from 2009 to 2014 and saving it in the processed folder\n",
    "raw_path = os.environ.get('RAW_FILE_PATH')\n",
    "cleaned_path = os.environ.get('PROCESSED_PATH')\n",
    "for excel in os.listdir(raw_path):\n",
    "    data = pd.read_csv(excel)\n",
    "    data.to_csv(os.path.join(cleaned_path, excel), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-18T16:19:37.089546",
     "start_time": "2016-09-18T16:19:23.967907"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combining it to a single dataframe\n",
    "data = pd.DataFrame()\n",
    "for i in range(2009, 2015):\n",
    "    filepath = '~/cpsc/data/processed/neiss/neiss-' + str(i) + '.csv'\n",
    "    temp = pd.read_csv(filepath, compression='gzip')\n",
    "    temp['year'] = i\n",
    "    data = pd.concat([data, temp])\n",
    "data['narr2'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-18T16:50:25.022050",
     "start_time": "2016-09-18T16:50:22.158264"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPSC Case #</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>body_part</th>\n",
       "      <th>diag</th>\n",
       "      <th>diag_other</th>\n",
       "      <th>disposition</th>\n",
       "      <th>fmv</th>\n",
       "      <th>location</th>\n",
       "      <th>narr1</th>\n",
       "      <th>...</th>\n",
       "      <th>prod1</th>\n",
       "      <th>prod2</th>\n",
       "      <th>psu</th>\n",
       "      <th>race</th>\n",
       "      <th>race_other</th>\n",
       "      <th>sex</th>\n",
       "      <th>stratum</th>\n",
       "      <th>trmt_date</th>\n",
       "      <th>weight</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90101432</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>89</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>Other / Mixed Race</td>\n",
       "      <td>HISPANIC</td>\n",
       "      <td>Male</td>\n",
       "      <td>V</td>\n",
       "      <td>01-01-09</td>\n",
       "      <td>15.3491</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90101434</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>77</td>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>V</td>\n",
       "      <td>01-01-09</td>\n",
       "      <td>15.3491</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90101435</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>76</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>V</td>\n",
       "      <td>01-01-09</td>\n",
       "      <td>15.3491</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90101436</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>93</td>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>V</td>\n",
       "      <td>01-01-09</td>\n",
       "      <td>15.3491</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90101437</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>V</td>\n",
       "      <td>01-01-09</td>\n",
       "      <td>15.3491</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CPSC Case #  Unnamed: 0  age  body_part  diag diag_other  disposition  fmv  \\\n",
       "0     90101432           0    5         89    64        NaN            1    0   \n",
       "1     90101434           1   51         77    53        NaN            1    0   \n",
       "2     90101435           2    2         76    59        NaN            1    0   \n",
       "3     90101436           3   20         93    53        NaN            1    0   \n",
       "4     90101437           4   20         34    57        NaN            1    0   \n",
       "\n",
       "   location narr1  ...  prod1 prod2  psu                race  race_other  \\\n",
       "0         1   NaN  ...   1807   NaN   61  Other / Mixed Race    HISPANIC   \n",
       "1         1   NaN  ...    899   NaN   61               White         NaN   \n",
       "2         1   NaN  ...   4057   NaN   61               White         NaN   \n",
       "3         1   NaN  ...   1884   NaN   61               White         NaN   \n",
       "4         9   NaN  ...   3283   NaN   61               White         NaN   \n",
       "\n",
       "      sex stratum trmt_date   weight  year  \n",
       "0    Male       V  01-01-09  15.3491  2009  \n",
       "1    Male       V  01-01-09  15.3491  2009  \n",
       "2  Female       V  01-01-09  15.3491  2009  \n",
       "3    Male       V  01-01-09  15.3491  2009  \n",
       "4    Male       V  01-01-09  15.3491  2009  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing the 2015 manually\n",
    "latest = pd.read_csv('~/cpsc/data/processed/neiss/neiss-2015.csv', compression='gzip')\n",
    "latest['year'] = '2015'\n",
    "columns = latest.columns.values.tolist()\n",
    "new_cols = columns[:-2]\n",
    "new_cols.append(columns[-1])\n",
    "new_cols.append(columns[-2])\n",
    "final = pd.concat([data, latest.ix[:,new_cols]])\n",
    "final.to_csv('~/cpsc/data/processed/neiss-combined.csv', compression='gzip')\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-18T16:50:57.848605",
     "start_time": "2016-09-18T16:50:57.637097"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010    405710\n",
       "2011    396502\n",
       "2012    394383\n",
       "2009    391944\n",
       "2013    376926\n",
       "2014    367492\n",
       "2015    359129\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick check\n",
    "final.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning of Analysis\n",
    "\n",
    "Just answering some of the questions the CPSC had on their data. I start with the hackpad. Here I open the cleaned api data using pickle since I saved it in a pickle format (saving it as a csv ran into encoding error and I didn't want to corrupt the data further)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-18T16:55:01.578864",
     "start_time": "2016-09-18T16:54:48.826747"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cipherpol/ipython/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (6,10,11,12,16,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# neiss = pd.read_csv('/NEISS-data-2015-updated-APRIL2016.csv')\n",
    "data = pickle.load(open('~/cpsc/data/processed/cleaned_api_data', 'rb'))\n",
    "neiss = pd.read_csv('~/cpsc/data/processed/neiss-combined.csv', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class parser(object):\n",
    "    \n",
    "    def __init__(self, items_list):\n",
    "        self.items_list = items_list\n",
    "        self.product_list = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def paren_split(item):\n",
    "        return item.split('(')[0]\n",
    "\n",
    "    def step_one(self, item):\n",
    "        if len(item.split(' ')) > 1:\n",
    "            parsed_items = []\n",
    "            if ')' in item:\n",
    "                temp_list = self.paren_split(item)\n",
    "            elif ',' in item:\n",
    "                temp_list = item.split(',')\n",
    "            else:\n",
    "                temp_list = item\n",
    "            return temp_list\n",
    "        else:\n",
    "            return item\n",
    "        \n",
    "    @staticmethod\n",
    "    def step_two(item):\n",
    "        if isinstance(item, list):\n",
    "            step_two_results = []\n",
    "            for each in item:\n",
    "                if ' or ' in each:\n",
    "                    results = each.split(' or ')\n",
    "                elif ' and ' in each:\n",
    "                    results = each.split(' and ')\n",
    "                elif ' & ' in each:\n",
    "                    results = each.split(' & ')\n",
    "                else:\n",
    "                    results = each\n",
    "                step_two_results.append(results)\n",
    "        else:\n",
    "            step_two_results = item\n",
    "        return step_two_results\n",
    "                    \n",
    "    def flatten(self):\n",
    "        temp = [item for sublist in self.parsed if isinstance(sublist, list) for item in sublist]\n",
    "        return [item.lower().strip() for sublist in temp for item in sublist if isinstance(sublist, list)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_boolean(item):\n",
    "        remove_criterias = ['other', 'not specified', ',', '.']\n",
    "        return any(criterion in item for criterion in remove_criterias)\n",
    "        \n",
    "    @staticmethod\n",
    "    def deduplicate_list(raw_list):\n",
    "        deduped = []\n",
    "        for i in raw_list:\n",
    "            if i not in deduped:\n",
    "                deduped.append(i)\n",
    "        return deduped\n",
    "    \n",
    "    def clean_up_list(self, item_list):\n",
    "        removed_list = [self.paren_split(item) for item in item_list if not self.remove_boolean(item) and item != '']\n",
    "        removed_list = self.deduplicate_list(removed_list)\n",
    "        self.cleaned = removed_list\n",
    "        return self.cleaned\n",
    "    \n",
    "    def run_parser(self):\n",
    "        results = []\n",
    "        for item in self.items_list:\n",
    "            parsed = self.step_one(item)\n",
    "            results.append(parsed)\n",
    "        next_step = []\n",
    "        for item in results:\n",
    "            parsed = self.step_two(item)\n",
    "            next_step.append(parsed)\n",
    "        self.parsed = next_step\n",
    "        return self.parsed\n",
    "\n",
    "    def post_parse(self):\n",
    "        flattened = self.flatten()\n",
    "        return self.clean_up_list(flattened)\n",
    "    \n",
    "    def run(self):\n",
    "        self.run_parser()\n",
    "        self.post_parse()\n",
    "        return self.cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = parser(products)\n",
    "test.run()[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = neiss.narr1[2]\n",
    "tokened = nltk.word_tokenize(text)\n",
    "print(text)\n",
    "nltk.pos_tag(tokened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = data.IncidentDescription[2]\n",
    "tokened = nltk.word_tokenize(text)\n",
    "print(text)\n",
    "nltk.pos_tag(tokened)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "899px",
   "left": "0px",
   "right": "1603px",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
