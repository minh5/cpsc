{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev3 toc-item\"><a href=\"#Using-the-CPSC-API-(unsuccessfully)\" data-toc-modified-id=\"Using-the-CPSC-API-(unsuccessfully)-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span>Using the CPSC API (unsuccessfully)</a></div><div class=\"lev3 toc-item\"><a href=\"#Cleaning-up-Raw-Data\" data-toc-modified-id=\"Cleaning-up-Raw-Data-0.0.2\"><span class=\"toc-item-num\">0.0.2&nbsp;&nbsp;</span>Cleaning up Raw Data</a></div><div class=\"lev3 toc-item\"><a href=\"#Beginning-of-Analysis\" data-toc-modified-id=\"Beginning-of-Analysis-0.0.3\"><span class=\"toc-item-num\">0.0.3&nbsp;&nbsp;</span>Beginning of Analysis</a></div><div class=\"lev3 toc-item\"><a href=\"#Natural-Language-Processing-Approach\" data-toc-modified-id=\"Natural-Language-Processing-Approach-0.0.4\"><span class=\"toc-item-num\">0.0.4&nbsp;&nbsp;</span>Natural Language Processing Approach</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-18T13:20:17.800144",
     "start_time": "2016-09-18T13:20:17.794482"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "import unirest\n",
    "import os\n",
    "\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the CPSC API (unsuccessfully)\n",
    "\n",
    "Here I try to make a request from the CPSC API and it returns a blank even though the status code is 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = os.environ.get('CPSC_KEY')\n",
    "resp = unirest.get('http://www.saferproducts.gov/webapi/Cpsc.Cpsrms.Web.Api.svc/',\n",
    "                   auth=(key,''), headers={\"Accept\": \"application/json\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up Raw Data\n",
    "\n",
    "Rather than calling from the API, we received the data from the epidemiologist from the CPSC in a raw `.txt` format. However it is a JSON file and there are levels of nesting that we would need to parse out so I created this script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('data/raw_api_data.txt')\n",
    "\n",
    "cols_to_parse = ['Gender', 'SeverityType', 'Locale', 'ProductCategory']\n",
    "cols_to_add = [['GenderDescription','GenderId','GenderPublicName'],\n",
    "['IncidentDetails','SeverityTypeDescription','SeverityTypePublicName'],\n",
    "['LocaleDescription','LocalePublicName'],\n",
    "['ProductCategoryDescription','ProductCategoryPublicName']]\n",
    "new_df = pd.DataFrame()\n",
    "cols_dict = dict(zip(cols_to_parse, cols_to_add))\n",
    "\n",
    "for key in cols_dict:\n",
    "    placeholder = pd.DataFrame(columns = cols_dict[key])\n",
    "    for (i, row) in data.iterrows():\n",
    "        e = row[key]\n",
    "        value_holder = []\n",
    "        for item in cols_dict[key]:\n",
    "            try:\n",
    "                component = e[item]\n",
    "            except:\n",
    "                component = 'Missing'\n",
    "            value_holder.append(component)\n",
    "        placeholder.loc[i, :] = value_holder\n",
    "    if new_df.shape[0] == 0:\n",
    "        new_df = placeholder\n",
    "    else:\n",
    "        new_df = pd.concat([new_df, placeholder], axis=1)\n",
    "        \n",
    "new_df2 = pd.concat([data, new_df], axis=1)\n",
    "new_df2 = new_df2.drop(['CompanyComments', 'Gender','IncidentDocuments','IncidentDetails', 'Locale', 'ProductCategory',\n",
    "                      'RelationshipType','SeverityType', 'SourceType'], axis=1)\n",
    "new_df2.to_pickle('data/cleaned_api_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-18T13:30:44.785593",
     "start_time": "2016-09-18T13:30:44.777690"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEISS-data-2015-updated-APRIL2016.xlsx',\n",
       " 'NEISS-data-2014-updated-12MAY2015.xlsx',\n",
       " 'NEISS-data-2009-updated-12MAY2015.xlsx',\n",
       " 'NEISS-data-2011-updated-12MAY2015.xlsx',\n",
       " 'NEISS-data-2013-updated-12MAY2015.xlsx',\n",
       " 'NEISS-data-2012-updated-12MAY2015.xlsx',\n",
       " 'NEISS-data-2010-updated-12MAY2015.xlsx']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data9 = pd.read_excel('../data/raw/NEISS/NEISS-data-2009-updated-12MAY2015.xlsx')\n",
    "NEISS = pd.DataFrame()\n",
    "working_path = os.path.join('/','home','datauser','cpsc','data','raw','NEISS')\n",
    "os.listdir(working_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-18T13:30:22.103283",
     "start_time": "2016-09-18T13:30:22.098974"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/datauser/cpsc/notebooks'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-18T13:30:34.977429",
     "start_time": "2016-09-18T13:30:34.973062"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'home/datauser/cpsc/data/raw/NEISS'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-18T13:17:40.630092",
     "start_time": "2016-09-18T13:17:40.480322"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEISS-data-2009-updated-12MAY2015.xlsx  NEISS-data-2013-updated-12MAY2015.xlsx\r\n",
      "NEISS-data-2010-updated-12MAY2015.xlsx  NEISS-data-2014-updated-12MAY2015.xlsx\r\n",
      "NEISS-data-2011-updated-12MAY2015.xlsx  NEISS-data-2015-updated-APRIL2016.xlsx\r\n",
      "NEISS-data-2012-updated-12MAY2015.xlsx\r\n"
     ]
    }
   ],
   "source": [
    "files = ['NEISS-data-2009-updated-12MAY2015.xlsx', 'NEISS-data-2013-updated-12MAY2015.xlsx',\n",
    "         'NEISS-data-2010-updated-12MAY2015.xlsx', 'NEISS-data-2014-updated-12MAY2015.xlsx',\n",
    "         'NEISS-data-2011-updated-12MAY2015.xlsx',  'NEISS-data-2015-updated-APRIL2016.xlsx',\n",
    "         'NEISS-data-2012-updated-12MAY2015.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.compile(r'NEISS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning of Analysis\n",
    "\n",
    "This is just a brute force way of parsing out the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-18T11:36:09.971909",
     "start_time": "2016-09-18T11:36:09.923524"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neiss = pd.read_csv('/NEISS-data-2015-updated-APRIL2016.csv')\n",
    "data = pickle.load(open('/home/datauser/cpsc/data/processed/cleaned_api_data', 'rb'))\n",
    "neiss = pd.read_csv('/home/datauser/cpsc/data/raw/NEISS-data-2015-updated-APRIL2016.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class parser(object):\n",
    "    \n",
    "    def __init__(self, items_list):\n",
    "        self.items_list = items_list\n",
    "        self.product_list = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def paren_split(item):\n",
    "        return item.split('(')[0]\n",
    "\n",
    "    def step_one(self, item):\n",
    "        if len(item.split(' ')) > 1:\n",
    "            parsed_items = []\n",
    "            if ')' in item:\n",
    "                temp_list = self.paren_split(item)\n",
    "            elif ',' in item:\n",
    "                temp_list = item.split(',')\n",
    "            else:\n",
    "                temp_list = item\n",
    "            return temp_list\n",
    "        else:\n",
    "            return item\n",
    "        \n",
    "    @staticmethod\n",
    "    def step_two(item):\n",
    "        if isinstance(item, list):\n",
    "            step_two_results = []\n",
    "            for each in item:\n",
    "                if ' or ' in each:\n",
    "                    results = each.split(' or ')\n",
    "                elif ' and ' in each:\n",
    "                    results = each.split(' and ')\n",
    "                elif ' & ' in each:\n",
    "                    results = each.split(' & ')\n",
    "                else:\n",
    "                    results = each\n",
    "                step_two_results.append(results)\n",
    "        else:\n",
    "            step_two_results = item\n",
    "        return step_two_results\n",
    "                    \n",
    "    def flatten(self):\n",
    "        temp = [item for sublist in self.parsed if isinstance(sublist, list) for item in sublist]\n",
    "        return [item.lower().strip() for sublist in temp for item in sublist if isinstance(sublist, list)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_boolean(item):\n",
    "        remove_criterias = ['other', 'not specified', ',', '.']\n",
    "        return any(criterion in item for criterion in remove_criterias)\n",
    "        \n",
    "    @staticmethod\n",
    "    def deduplicate_list(raw_list):\n",
    "        deduped = []\n",
    "        for i in raw_list:\n",
    "            if i not in deduped:\n",
    "                deduped.append(i)\n",
    "        return deduped\n",
    "    \n",
    "    def clean_up_list(self, item_list):\n",
    "        removed_list = [self.paren_split(item) for item in item_list if not self.remove_boolean(item) and item != '']\n",
    "        removed_list = self.deduplicate_list(removed_list)\n",
    "        self.cleaned = removed_list\n",
    "        return self.cleaned\n",
    "    \n",
    "    def run_parser(self):\n",
    "        results = []\n",
    "        for item in self.items_list:\n",
    "            parsed = self.step_one(item)\n",
    "            results.append(parsed)\n",
    "        next_step = []\n",
    "        for item in results:\n",
    "            parsed = self.step_two(item)\n",
    "            next_step.append(parsed)\n",
    "        self.parsed = next_step\n",
    "        return self.parsed\n",
    "\n",
    "    def post_parse(self):\n",
    "        flattened = self.flatten()\n",
    "        return self.clean_up_list(flattened)\n",
    "    \n",
    "    def run(self):\n",
    "        self.run_parser()\n",
    "        self.post_parse()\n",
    "        return self.cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = parser(products)\n",
    "test.run()[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = neiss.narr1[2]\n",
    "tokened = nltk.word_tokenize(text)\n",
    "print(text)\n",
    "nltk.pos_tag(tokened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = data.IncidentDescription[2]\n",
    "tokened = nltk.word_tokenize(text)\n",
    "print(text)\n",
    "nltk.pos_tag(tokened)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "899px",
   "left": "0px",
   "right": "1603px",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
