{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nltk",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-13ac9e49e7bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munirest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named nltk"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "import unirest\n",
    "import os\n",
    "\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (unsuccessfully) Using the CPSC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = os.environ.get('CPSC_KEY')\n",
    "resp = unirest.get('http://www.saferproducts.gov/webapi/Cpsc.Cpsrms.Web.Api.svc/',\n",
    "                   auth=(key,''), headers={\"Accept\": \"application/json\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('data/raw_api_data.txt')\n",
    "\n",
    "cols_to_parse = ['Gender', 'SeverityType', 'Locale', 'ProductCategory']\n",
    "cols_to_add = [['GenderDescription','GenderId','GenderPublicName'],\n",
    "['IncidentDetails','SeverityTypeDescription','SeverityTypePublicName'],\n",
    "['LocaleDescription','LocalePublicName'],\n",
    "['ProductCategoryDescription','ProductCategoryPublicName']]\n",
    "new_df = pd.DataFrame()\n",
    "cols_dict = dict(zip(cols_to_parse, cols_to_add))\n",
    "\n",
    "for key in cols_dict:\n",
    "    placeholder = pd.DataFrame(columns = cols_dict[key])\n",
    "    for (i, row) in data.iterrows():\n",
    "        e = row[key]\n",
    "        value_holder = []\n",
    "        for item in cols_dict[key]:\n",
    "            try:\n",
    "                component = e[item]\n",
    "            except:\n",
    "                component = 'Missing'\n",
    "            value_holder.append(component)\n",
    "        placeholder.loc[i, :] = value_holder\n",
    "    if new_df.shape[0] == 0:\n",
    "        new_df = placeholder\n",
    "    else:\n",
    "        new_df = pd.concat([new_df, placeholder], axis=1)\n",
    "        \n",
    "new_df2 = pd.concat([data, new_df], axis=1)\n",
    "new_df2 = new_df2.drop(['CompanyComments', 'Gender','IncidentDocuments','IncidentDetails', 'Locale', 'ProductCategory',\n",
    "                      'RelationshipType','SeverityType', 'SourceType'], axis=1)\n",
    "new_df2.to_pickle('data/cleaned_api_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning of Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neiss = pd.read_csv('/NEISS-data-2015-updated-APRIL2016.csv')\n",
    "data = pickle.load(open('/home/datauser/cpsc/data/cleaned_api_data', 'rb'))\n",
    "neiss = pd.read_csv('/home/datauser/cpsc/data/NEISS-data-2015-updated-APRIL2016.csv')\n",
    "\n",
    "products = data.ProductCategoryPublicName.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Hair Curlers, Curling Irons, Clips & Hairpins\n",
       "1                                                 Cribs\n",
       "2     Electric Ranges or Ovens (Excl Counter-top Ovens)\n",
       "3                                         Refrigerators\n",
       "4                                               Diapers\n",
       "5                                           Pogo Sticks\n",
       "8                           Coal or Wood-burning Stoves\n",
       "9                                           Televisions\n",
       "10       Candles, Candlesticks and Other Candle Holders\n",
       "11                                Sheets or Pillowcases\n",
       "12                               Baby Gates or Barriers\n",
       "13                                             Footwear\n",
       "15                                 Clothing Accessories\n",
       "16                                            Batteries\n",
       "18                                          Light Bulbs\n",
       "19                                          Dishwashers\n",
       "20                              Toy Musical Instruments\n",
       "21                              Lighted Make-up Mirrors\n",
       "24    Infant & Toddler Play Ctrs, Excl Jumpers,bounc...\n",
       "25           Computers (Equipment and Electronic Games)\n",
       "Name: ProductCategoryPublicName, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class parser(object):\n",
    "    \n",
    "    def __init__(self, items_list):\n",
    "        self.items_list = items_list\n",
    "        self.product_list = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def paren_split(item):\n",
    "        return item.split('(')[0]\n",
    "\n",
    "    def step_one(self, item):\n",
    "        if len(item.split(' ')) > 1:\n",
    "            parsed_items = []\n",
    "            if ')' in item:\n",
    "                temp_list = self.paren_split(item)\n",
    "            elif ',' in item:\n",
    "                temp_list = item.split(',')\n",
    "            else:\n",
    "                temp_list = item\n",
    "            return temp_list\n",
    "        else:\n",
    "            return item\n",
    "        \n",
    "    @staticmethod\n",
    "    def step_two(item):\n",
    "        if isinstance(item, list):\n",
    "            step_two_results = []\n",
    "            for each in item:\n",
    "                if ' or ' in each:\n",
    "                    results = each.split(' or ')\n",
    "                elif ' and ' in each:\n",
    "                    results = each.split(' and ')\n",
    "                elif ' & ' in each:\n",
    "                    results = each.split(' & ')\n",
    "                else:\n",
    "                    results = each\n",
    "                step_two_results.append(results)\n",
    "        else:\n",
    "            step_two_results = item\n",
    "        return step_two_results\n",
    "                    \n",
    "    def flatten(self):\n",
    "        temp = [item for sublist in self.parsed if isinstance(sublist, list) for item in sublist]\n",
    "        return [item.lower().strip() for sublist in temp for item in sublist if isinstance(sublist, list)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_boolean(item):\n",
    "        remove_criterias = ['other', 'not specified', ',', '.']\n",
    "        return any(criterion in item for criterion in remove_criterias)\n",
    "        \n",
    "    @staticmethod\n",
    "    def deduplicate_list(raw_list):\n",
    "        deduped = []\n",
    "        for i in raw_list:\n",
    "            if i not in deduped:\n",
    "                deduped.append(i)\n",
    "        return deduped\n",
    "    \n",
    "    def clean_up_list(self, item_list):\n",
    "        removed_list = [self.paren_split(item) for item in item_list if not self.remove_boolean(item) and item != '']\n",
    "        removed_list = self.deduplicate_list(removed_list)\n",
    "        self.cleaned = removed_list\n",
    "        return self.cleaned\n",
    "    \n",
    "    def run_parser(self):\n",
    "        results = []\n",
    "        for item in self.items_list:\n",
    "            parsed = self.step_one(item)\n",
    "            results.append(parsed)\n",
    "        next_step = []\n",
    "        for item in results:\n",
    "            parsed = self.step_two(item)\n",
    "            next_step.append(parsed)\n",
    "        self.parsed = next_step\n",
    "        return self.parsed\n",
    "\n",
    "    def post_parse(self):\n",
    "        flattened = self.flatten()\n",
    "        return self.clean_up_list(flattened)\n",
    "    \n",
    "    def run(self):\n",
    "        self.run_parser()\n",
    "        self.post_parse()\n",
    "        return self.cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hairpins',\n",
       " u'candlesticks',\n",
       " u'infant',\n",
       " u'toddler play ctrs',\n",
       " u'ranges',\n",
       " u'ovens',\n",
       " u'action figures',\n",
       " u'coffee makers',\n",
       " u'teapots',\n",
       " u'stacking toys',\n",
       " u'pull toys',\n",
       " u'gas',\n",
       " u'lp heaters',\n",
       " u'broilers',\n",
       " u'toaster ovens',\n",
       " u'divans',\n",
       " u'studio couches',\n",
       " u'housewares',\n",
       " u'appliances']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = parser(products)\n",
    "test.run()[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 YO F C/O EAR PAIN 1 DAY SAS WAS SWIMMING YESTERDAY NOTICED DISCOMFOR\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('20', 'CD'),\n",
       " ('YO', 'NNP'),\n",
       " ('F', 'NNP'),\n",
       " ('C/O', 'NNP'),\n",
       " ('EAR', 'NNP'),\n",
       " ('PAIN', 'NNP'),\n",
       " ('1', 'CD'),\n",
       " ('DAY', 'NNP'),\n",
       " ('SAS', 'NNP'),\n",
       " ('WAS', 'NNP'),\n",
       " ('SWIMMING', 'NNP'),\n",
       " ('YESTERDAY', 'NNP'),\n",
       " ('NOTICED', 'NNP'),\n",
       " ('DISCOMFOR', 'NNP')]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = neiss.narr1[2]\n",
    "tokened = nltk.word_tokenize(text)\n",
    "print(text)\n",
    "nltk.pos_tag(tokened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a Frigidaire electric range that comes on without being turned on and the only way to get it to go off is to unplug it.  It is very unsafe and I'm afraid to leave in plugged in when it's not in use.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'I', 'PRP'),\n",
       " (u'have', 'VBP'),\n",
       " (u'a', 'DT'),\n",
       " (u'Frigidaire', 'NNP'),\n",
       " (u'electric', 'JJ'),\n",
       " (u'range', 'NN'),\n",
       " (u'that', 'WDT'),\n",
       " (u'comes', 'VBZ'),\n",
       " (u'on', 'IN'),\n",
       " (u'without', 'IN'),\n",
       " (u'being', 'VBG'),\n",
       " (u'turned', 'VBN'),\n",
       " (u'on', 'IN'),\n",
       " (u'and', 'CC'),\n",
       " (u'the', 'DT'),\n",
       " (u'only', 'JJ'),\n",
       " (u'way', 'NN'),\n",
       " (u'to', 'TO'),\n",
       " (u'get', 'VB'),\n",
       " (u'it', 'PRP'),\n",
       " (u'to', 'TO'),\n",
       " (u'go', 'VB'),\n",
       " (u'off', 'RP'),\n",
       " (u'is', 'VBZ'),\n",
       " (u'to', 'TO'),\n",
       " (u'unplug', 'VB'),\n",
       " (u'it', 'PRP'),\n",
       " (u'.', '.'),\n",
       " (u'It', 'PRP'),\n",
       " (u'is', 'VBZ'),\n",
       " (u'very', 'RB'),\n",
       " (u'unsafe', 'JJ'),\n",
       " (u'and', 'CC'),\n",
       " (u'I', 'PRP'),\n",
       " (u\"'m\", 'VBP'),\n",
       " (u'afraid', 'JJ'),\n",
       " (u'to', 'TO'),\n",
       " (u'leave', 'VB'),\n",
       " (u'in', 'IN'),\n",
       " (u'plugged', 'VBN'),\n",
       " (u'in', 'IN'),\n",
       " (u'when', 'WRB'),\n",
       " (u'it', 'PRP'),\n",
       " (u\"'s\", 'VBZ'),\n",
       " (u'not', 'RB'),\n",
       " (u'in', 'IN'),\n",
       " (u'use', 'NN'),\n",
       " (u'.', '.')]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data.IncidentDescription[2]\n",
    "tokened = nltk.word_tokenize(text)\n",
    "print(text)\n",
    "nltk.pos_tag(tokened)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
